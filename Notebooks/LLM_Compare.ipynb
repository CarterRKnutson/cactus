{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a54d85",
   "metadata": {},
   "source": [
    "# Introduction to Large Language Models and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf9d936",
   "metadata": {},
   "source": [
    "This notebook will walk you though some simple steps for loading LLM's from `HuggingFace` and loading them into local pipelines for use with `LangChain`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19103f87",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Below are some basic imports that will be required to run the models, we are looking for `torch`, `transformers` and `langchain` as the main package libraries. We will import more from these packages later, but these are what we will need to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60408a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, pipeline,LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841cc798",
   "metadata": {},
   "source": [
    "Something I like to do is have a re-runnable cell that calls `nvidia-smi`. This allows us to see the type of GPU we are currently running on, as well as seeing how much available memory we have in case we want to load more LLMs to compare with eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e437efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0885263",
   "metadata": {},
   "source": [
    "Next we want to show which version of LangChain we are running, since this package is under constant development we want to make sure we are aware of which version we are running in case of version incompatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ad13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd16fff4",
   "metadata": {},
   "source": [
    "## Load in the Models through HuggingFace Pipelines\n",
    "\n",
    "Now that we have the initial imports, we can start downloading `HuggingFace` models locally to run by using the HuggingFace `transformers` package. This gives us access to the `AutoModel` methods such as: `AutoTokenizer` and `AutoModelForCausalLM`. We will need these to load in our text generation models as LLM Pipelines.\n",
    "\n",
    "Below I am providing an example model with commented steps for importing the model as well as loading it into `LangChain`. Then I will provide a few more models that we may want to further explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5413bcc1",
   "metadata": {},
   "source": [
    "### EXAMPLE: facebook/opt-13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df86981",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id='facebook/opt-13b' # Here we list the model_id that is given for the model on HuggingFace. (https://huggingface.co/facebook/opt-13b)\n",
    "cache_path = '../Models' # This will be a relative path to the directory which we will store model weights.\n",
    "\n",
    "# Here we import the tokenizer for the opt-13b model, we use the AutoTokenizer.from_pretrained() to load the weights from HuggingFace.\n",
    "opt_tokenizer = AutoTokenizer.from_pretrained(model_id,cache_dir=cache_path) \n",
    "\n",
    "# Now we load the actual LLM. We do this a similar way to the Tokenizer, but use AutoModelForCausalLM.from_pretrained(). We also specify that we want to load the model in 8bit with load_in_8bit=True\n",
    "opt_model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                                 load_in_8bit=True,\n",
    "                                                 device_map='auto',\n",
    "                                                 cache_dir=cache_path)\n",
    "# IMPORTANT: For this to work, you need to ensure you have bitsandbytes as well as accelerate downloaded in your environment.\n",
    "\n",
    "# Now that we have a model and tokenizer loaded into memory, we can tie the 2 together using a HuggingFace Pipeline.\n",
    "# We need to establish the role of the model as a text-generation model and pass in some model arguments. There are more arguments than just max_length, but for simple model runs this is sufficient.\n",
    "opt_pipe = pipeline('text-generation',\n",
    "                 model=opt_model,\n",
    "                 tokenizer=opt_tokenizer,\n",
    "                 max_length=512)\n",
    "\n",
    "# Once our model is built into a pipe, we can use LangChain's HuggingFacePipeline method to wrap the HuggingFace pipe class we created into LangChain\n",
    "opt_llm = HuggingFacePipeline(pipeline=opt_pipe)\n",
    "opt_llm.model_id = model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e95109",
   "metadata": {},
   "source": [
    "### chavinlo/alpaca-native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac4231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"chavinlo/alpaca-native\"\n",
    "alpaca_tokenizer = LlamaTokenizer.from_pretrained(model_id, cache_dir='./Models')\n",
    "alpaca_model = LlamaForCausalLM.from_pretrained(model_id,\n",
    "                                              load_in_8bit=True,\n",
    "                                              device_map='auto',\n",
    "                                              cache_dir='./Models/')\n",
    "alpaca_pipe = pipeline('text-generation',\n",
    "                 model=alpaca_model,\n",
    "                 tokenizer=alpaca_tokenizer,\n",
    "                 max_length=512)\n",
    "alpaca_llm = HuggingFacePipeline(pipeline=alpaca_pipe)\n",
    "alpaca_llm.model_id = model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bc96bd",
   "metadata": {},
   "source": [
    "### chavinlo/alpaca-13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266de8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"chavinlo/alpaca-13b\"\n",
    "alpaca13b_tokenizer = LlamaTokenizer.from_pretrained(model_id, cache_dir='../Models')\n",
    "alpaca13b_model = LlamaForCausalLM.from_pretrained(model_id,\n",
    "                                              load_in_8bit=True,\n",
    "                                              device_map='auto',\n",
    "                                              cache_dir='../Models/')\n",
    "alpaca13b_pipe = pipeline('text-generation',\n",
    "                 model=alpaca13b_model,\n",
    "                 tokenizer=alpaca13b_tokenizer,\n",
    "                 max_length=512)\n",
    "alpaca13b_llm = HuggingFacePipeline(pipeline=alpaca13b_pipe)\n",
    "alpaca13b_llm.model_id = model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e7234",
   "metadata": {},
   "source": [
    "### facebook/galactica-6.7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e2470",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"facebook/galactica-6.7b\"\n",
    "\n",
    "galactica_tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir='../Models/')\n",
    "\n",
    "galactica_model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                       load_in_8bit=True,\n",
    "                                                       device_map='auto',\n",
    "                                                       cache_dir='../Models/'\n",
    "                                                      )\n",
    "\n",
    "galactica_pipe = pipeline('text-generation',\n",
    "                       model=galactica_model,\n",
    "                       tokenizer=galactica_tokenizer,\n",
    "                       max_length=512\n",
    "                      )\n",
    "\n",
    "galactica_llm = HuggingFacePipeline(pipeline=galactica_pipe)\n",
    "galactica_llm.model_id = model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4696c6",
   "metadata": {},
   "source": [
    "### GeorgiaTechResearchInstitute/galpaca-6.7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6770b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"GeorgiaTechResearchInstitute/galpaca-6.7b\"\n",
    "\n",
    "galpaca_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "galpaca_model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                      load_in_8bit=True,\n",
    "                                                      device_map='auto',\n",
    "                                                      cache_dir='../Models/'\n",
    "                                                      )\n",
    "\n",
    "galpaca_pipe = pipeline('text-generation',\n",
    "                       model=galpaca_model,\n",
    "                       tokenizer=galpaca_tokenizer,\n",
    "                       max_length=512\n",
    "                       )\n",
    "\n",
    "galpaca_llm = HuggingFacePipeline(pipeline=galpaca_pipe)\n",
    "galpaca_llm.model_id = model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00546bf3",
   "metadata": {},
   "source": [
    "## Running Basic Prompts through the LLMs\n",
    "\n",
    "Now that we have a few LLMs to choose from, we can use them for basic prompting. For this we will use LangChain's `PromptTemplate` along with LangChain's `LLMChain` to chain together the prompt template with the LLM. We use this for simple repeatable runs of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc58fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction: \n",
    "{instruction}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"instruction\"])\n",
    "\n",
    "llm_chain = LLMChain(llm=alpaca13b_llm, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c8d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"What is the capital of England?\"\n",
    "text = llm_chain.run(instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25a9480",
   "metadata": {},
   "source": [
    "## Using Tools and Agents\n",
    "\n",
    "Another use of the LLMs is the ability to run them as `Agents` with access to `Tools`. This will allow the LLMs to return data that is beyond the training scope of the models themselves. This allows the LLM to plan and execute a path towards solving the given input.\n",
    "\n",
    "For this, I will use an example of querying the LLM for the amount of heavy atoms present in a given compound. If I ask a question such as: `How many heavy atoms are in caffeine`, the LLM may not know the answer but if I have provided it a tool to find the answer, it will use that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708729d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.tools import DuckDuckGoSearchTool\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "import pubchempy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb5c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heavy_atom_counter(compound: str) -> int:\n",
    "    '''\n",
    "    mockup function: given a compound name,\n",
    "    return heavy atom count as an integer.\n",
    "\n",
    "    parameters:\n",
    "        compound: a chemical compound name\n",
    "\n",
    "    returns:\n",
    "        heavy atom count\n",
    "    '''\n",
    "    Query_Compound = pubchempy.get_compounds(str(compound),'name')[0]\n",
    "    Heavy_Atom_Count = Query_Compound.heavy_atom_count\n",
    "    \n",
    "    return Heavy_Atom_Count\n",
    "\n",
    "Heavy_Atom = Tool(\n",
    "    name=\"heavy_atom_counter\",\n",
    "    func=heavy_atom_counter,\n",
    "    #description=\"\"\"helps to retrieve the number of heavy atoms present in a compound. input should be json in the following format: `{{\"compound\": '<compound_name>'}}`\"\"\"\n",
    "    description=\"helps to retrieve the number of heavy atoms present in a compound. Input should be a string.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90fffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[],\n",
    "    template=\"Answer the following questions as best you can.\"\n",
    ")\n",
    "\n",
    "# Load the tool configs that are needed.\n",
    "llm_atom_chain = LLMChain(\n",
    "    llm=alpaca13b_llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    Heavy_Atom\n",
    "]\n",
    "\n",
    "# Construct the react agent type.\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    alpaca13b_llm,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run(\"How many heavy atoms in Atorvastatin?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
